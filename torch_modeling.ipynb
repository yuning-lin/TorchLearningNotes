{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6e857f",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "1. Design model(input, output, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop\n",
    "    * forward pass: compute prediction\n",
    "    * backward pass: compute gradient\n",
    "    * update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59aa5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff9f146",
   "metadata": {},
   "source": [
    "### 自定義 forward、weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026892ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "w = torch.tensor(0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iter = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5317565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return w*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77dad80a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000\n",
      "epoch 2: w = 0.555, loss = 21.67500\n",
      "epoch 3: w = 0.772, loss = 15.66019\n",
      "epoch 4: w = 0.956, loss = 11.31449\n",
      "epoch 5: w = 1.113, loss = 8.17472\n",
      "epoch 6: w = 1.246, loss = 5.90623\n",
      "epoch 7: w = 1.359, loss = 4.26725\n",
      "epoch 8: w = 1.455, loss = 3.08309\n",
      "epoch 9: w = 1.537, loss = 2.22753\n",
      "epoch 10: w = 1.606, loss = 1.60939\n",
      "epoch 11: w = 1.665, loss = 1.16279\n",
      "epoch 12: w = 1.716, loss = 0.84011\n",
      "epoch 13: w = 1.758, loss = 0.60698\n",
      "epoch 14: w = 1.794, loss = 0.43854\n",
      "epoch 15: w = 1.825, loss = 0.31685\n",
      "epoch 16: w = 1.851, loss = 0.22892\n",
      "epoch 17: w = 1.874, loss = 0.16540\n",
      "epoch 18: w = 1.893, loss = 0.11950\n",
      "epoch 19: w = 1.909, loss = 0.08634\n",
      "epoch 20: w = 1.922, loss = 0.06238\n",
      "epoch 21: w = 1.934, loss = 0.04507\n",
      "epoch 22: w = 1.944, loss = 0.03256\n",
      "epoch 23: w = 1.952, loss = 0.02353\n",
      "epoch 24: w = 1.960, loss = 0.01700\n",
      "epoch 25: w = 1.966, loss = 0.01228\n",
      "epoch 26: w = 1.971, loss = 0.00887\n",
      "epoch 27: w = 1.975, loss = 0.00641\n",
      "epoch 28: w = 1.979, loss = 0.00463\n",
      "epoch 29: w = 1.982, loss = 0.00335\n",
      "epoch 30: w = 1.985, loss = 0.00242\n",
      "epoch 31: w = 1.987, loss = 0.00175\n",
      "epoch 32: w = 1.989, loss = 0.00126\n",
      "epoch 33: w = 1.991, loss = 0.00091\n",
      "epoch 34: w = 1.992, loss = 0.00066\n",
      "epoch 35: w = 1.993, loss = 0.00048\n",
      "epoch 36: w = 1.994, loss = 0.00034\n",
      "epoch 37: w = 1.995, loss = 0.00025\n",
      "epoch 38: w = 1.996, loss = 0.00018\n",
      "epoch 39: w = 1.996, loss = 0.00013\n",
      "epoch 40: w = 1.997, loss = 0.00009\n",
      "epoch 41: w = 1.997, loss = 0.00007\n",
      "epoch 42: w = 1.998, loss = 0.00005\n",
      "epoch 43: w = 1.998, loss = 0.00004\n",
      "epoch 44: w = 1.998, loss = 0.00003\n",
      "epoch 45: w = 1.999, loss = 0.00002\n",
      "epoch 46: w = 1.999, loss = 0.00001\n",
      "epoch 47: w = 1.999, loss = 0.00001\n",
      "epoch 48: w = 1.999, loss = 0.00001\n",
      "epoch 49: w = 1.999, loss = 0.00001\n",
      "epoch 50: w = 1.999, loss = 0.00000\n",
      "epoch 51: w = 1.999, loss = 0.00000\n",
      "epoch 52: w = 2.000, loss = 0.00000\n",
      "epoch 53: w = 2.000, loss = 0.00000\n",
      "epoch 54: w = 2.000, loss = 0.00000\n",
      "epoch 55: w = 2.000, loss = 0.00000\n",
      "epoch 56: w = 2.000, loss = 0.00000\n",
      "epoch 57: w = 2.000, loss = 0.00000\n",
      "epoch 58: w = 2.000, loss = 0.00000\n",
      "epoch 59: w = 2.000, loss = 0.00000\n",
      "epoch 60: w = 2.000, loss = 0.00000\n",
      "epoch 61: w = 2.000, loss = 0.00000\n",
      "epoch 62: w = 2.000, loss = 0.00000\n",
      "epoch 63: w = 2.000, loss = 0.00000\n",
      "epoch 64: w = 2.000, loss = 0.00000\n",
      "epoch 65: w = 2.000, loss = 0.00000\n",
      "epoch 66: w = 2.000, loss = 0.00000\n",
      "epoch 67: w = 2.000, loss = 0.00000\n",
      "epoch 68: w = 2.000, loss = 0.00000\n",
      "epoch 69: w = 2.000, loss = 0.00000\n",
      "epoch 70: w = 2.000, loss = 0.00000\n",
      "epoch 71: w = 2.000, loss = 0.00000\n",
      "epoch 72: w = 2.000, loss = 0.00000\n",
      "epoch 73: w = 2.000, loss = 0.00000\n",
      "epoch 74: w = 2.000, loss = 0.00000\n",
      "epoch 75: w = 2.000, loss = 0.00000\n",
      "epoch 76: w = 2.000, loss = 0.00000\n",
      "epoch 77: w = 2.000, loss = 0.00000\n",
      "epoch 78: w = 2.000, loss = 0.00000\n",
      "epoch 79: w = 2.000, loss = 0.00000\n",
      "epoch 80: w = 2.000, loss = 0.00000\n",
      "epoch 81: w = 2.000, loss = 0.00000\n",
      "epoch 82: w = 2.000, loss = 0.00000\n",
      "epoch 83: w = 2.000, loss = 0.00000\n",
      "epoch 84: w = 2.000, loss = 0.00000\n",
      "epoch 85: w = 2.000, loss = 0.00000\n",
      "epoch 86: w = 2.000, loss = 0.00000\n",
      "epoch 87: w = 2.000, loss = 0.00000\n",
      "epoch 88: w = 2.000, loss = 0.00000\n",
      "epoch 89: w = 2.000, loss = 0.00000\n",
      "epoch 90: w = 2.000, loss = 0.00000\n",
      "epoch 91: w = 2.000, loss = 0.00000\n",
      "epoch 92: w = 2.000, loss = 0.00000\n",
      "epoch 93: w = 2.000, loss = 0.00000\n",
      "epoch 94: w = 2.000, loss = 0.00000\n",
      "epoch 95: w = 2.000, loss = 0.00000\n",
      "epoch 96: w = 2.000, loss = 0.00000\n",
      "epoch 97: w = 2.000, loss = 0.00000\n",
      "epoch 98: w = 2.000, loss = 0.00000\n",
      "epoch 99: w = 2.000, loss = 0.00000\n",
      "epoch 100: w = 2.000, loss = 0.00000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    # forward pass\n",
    "    Y_pred = forward(X)\n",
    "    l = loss(Y,Y_pred)\n",
    "    # backward pass\n",
    "    l.backward()\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    # zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch%1 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.5f}')\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec415c",
   "metadata": {},
   "source": [
    "### 選定模型：Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bba353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([[5]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size) # get weight\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iter = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99cf7567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 3.966\n",
      "epoch 1: w = 1.020, loss = 11.43904\n",
      "epoch 2: w = 1.174, loss = 7.93873\n",
      "epoch 3: w = 1.303, loss = 5.50993\n",
      "epoch 4: w = 1.410, loss = 3.82463\n",
      "epoch 5: w = 1.499, loss = 2.65523\n",
      "epoch 6: w = 1.574, loss = 1.84380\n",
      "epoch 7: w = 1.636, loss = 1.28075\n",
      "epoch 8: w = 1.687, loss = 0.89006\n",
      "epoch 9: w = 1.730, loss = 0.61896\n",
      "epoch 10: w = 1.766, loss = 0.43084\n",
      "epoch 11: w = 1.796, loss = 0.30030\n",
      "epoch 12: w = 1.821, loss = 0.20971\n",
      "epoch 13: w = 1.842, loss = 0.14685\n",
      "epoch 14: w = 1.859, loss = 0.10322\n",
      "epoch 15: w = 1.874, loss = 0.07294\n",
      "epoch 16: w = 1.886, loss = 0.05192\n",
      "epoch 17: w = 1.896, loss = 0.03733\n",
      "epoch 18: w = 1.904, loss = 0.02719\n",
      "epoch 19: w = 1.911, loss = 0.02015\n",
      "epoch 20: w = 1.917, loss = 0.01526\n",
      "epoch 21: w = 1.922, loss = 0.01186\n",
      "epoch 22: w = 1.926, loss = 0.00949\n",
      "epoch 23: w = 1.930, loss = 0.00784\n",
      "epoch 24: w = 1.933, loss = 0.00669\n",
      "epoch 25: w = 1.935, loss = 0.00588\n",
      "epoch 26: w = 1.937, loss = 0.00531\n",
      "epoch 27: w = 1.939, loss = 0.00491\n",
      "epoch 28: w = 1.941, loss = 0.00463\n",
      "epoch 29: w = 1.942, loss = 0.00442\n",
      "epoch 30: w = 1.943, loss = 0.00427\n",
      "epoch 31: w = 1.944, loss = 0.00416\n",
      "epoch 32: w = 1.945, loss = 0.00408\n",
      "epoch 33: w = 1.945, loss = 0.00401\n",
      "epoch 34: w = 1.946, loss = 0.00396\n",
      "epoch 35: w = 1.947, loss = 0.00391\n",
      "epoch 36: w = 1.947, loss = 0.00388\n",
      "epoch 37: w = 1.947, loss = 0.00384\n",
      "epoch 38: w = 1.948, loss = 0.00381\n",
      "epoch 39: w = 1.948, loss = 0.00379\n",
      "epoch 40: w = 1.948, loss = 0.00376\n",
      "epoch 41: w = 1.949, loss = 0.00374\n",
      "epoch 42: w = 1.949, loss = 0.00371\n",
      "epoch 43: w = 1.949, loss = 0.00369\n",
      "epoch 44: w = 1.949, loss = 0.00367\n",
      "epoch 45: w = 1.950, loss = 0.00364\n",
      "epoch 46: w = 1.950, loss = 0.00362\n",
      "epoch 47: w = 1.950, loss = 0.00360\n",
      "epoch 48: w = 1.950, loss = 0.00358\n",
      "epoch 49: w = 1.950, loss = 0.00356\n",
      "epoch 50: w = 1.951, loss = 0.00354\n",
      "epoch 51: w = 1.951, loss = 0.00351\n",
      "epoch 52: w = 1.951, loss = 0.00349\n",
      "epoch 53: w = 1.951, loss = 0.00347\n",
      "epoch 54: w = 1.951, loss = 0.00345\n",
      "epoch 55: w = 1.951, loss = 0.00343\n",
      "epoch 56: w = 1.952, loss = 0.00341\n",
      "epoch 57: w = 1.952, loss = 0.00339\n",
      "epoch 58: w = 1.952, loss = 0.00337\n",
      "epoch 59: w = 1.952, loss = 0.00335\n",
      "epoch 60: w = 1.952, loss = 0.00333\n",
      "epoch 61: w = 1.952, loss = 0.00331\n",
      "epoch 62: w = 1.952, loss = 0.00329\n",
      "epoch 63: w = 1.953, loss = 0.00327\n",
      "epoch 64: w = 1.953, loss = 0.00325\n",
      "epoch 65: w = 1.953, loss = 0.00323\n",
      "epoch 66: w = 1.953, loss = 0.00321\n",
      "epoch 67: w = 1.953, loss = 0.00319\n",
      "epoch 68: w = 1.953, loss = 0.00317\n",
      "epoch 69: w = 1.953, loss = 0.00315\n",
      "epoch 70: w = 1.954, loss = 0.00314\n",
      "epoch 71: w = 1.954, loss = 0.00312\n",
      "epoch 72: w = 1.954, loss = 0.00310\n",
      "epoch 73: w = 1.954, loss = 0.00308\n",
      "epoch 74: w = 1.954, loss = 0.00306\n",
      "epoch 75: w = 1.954, loss = 0.00304\n",
      "epoch 76: w = 1.954, loss = 0.00302\n",
      "epoch 77: w = 1.955, loss = 0.00301\n",
      "epoch 78: w = 1.955, loss = 0.00299\n",
      "epoch 79: w = 1.955, loss = 0.00297\n",
      "epoch 80: w = 1.955, loss = 0.00295\n",
      "epoch 81: w = 1.955, loss = 0.00294\n",
      "epoch 82: w = 1.955, loss = 0.00292\n",
      "epoch 83: w = 1.955, loss = 0.00290\n",
      "epoch 84: w = 1.955, loss = 0.00288\n",
      "epoch 85: w = 1.956, loss = 0.00287\n",
      "epoch 86: w = 1.956, loss = 0.00285\n",
      "epoch 87: w = 1.956, loss = 0.00283\n",
      "epoch 88: w = 1.956, loss = 0.00281\n",
      "epoch 89: w = 1.956, loss = 0.00280\n",
      "epoch 90: w = 1.956, loss = 0.00278\n",
      "epoch 91: w = 1.956, loss = 0.00276\n",
      "epoch 92: w = 1.957, loss = 0.00275\n",
      "epoch 93: w = 1.957, loss = 0.00273\n",
      "epoch 94: w = 1.957, loss = 0.00272\n",
      "epoch 95: w = 1.957, loss = 0.00270\n",
      "epoch 96: w = 1.957, loss = 0.00268\n",
      "epoch 97: w = 1.957, loss = 0.00267\n",
      "epoch 98: w = 1.957, loss = 0.00265\n",
      "epoch 99: w = 1.957, loss = 0.00263\n",
      "epoch 100: w = 1.958, loss = 0.00262\n",
      "Prediction after training: f(5) = 9.913\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    # forward pass\n",
    "    Y_pred = model(X)\n",
    "    l = loss(Y,Y_pred)\n",
    "    # backward pass\n",
    "    l.backward()\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    # zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch%1 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.5f}')\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8747f18",
   "metadata": {},
   "source": [
    "### 客製化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a27553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b3b9960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([[5]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = LinearRegression(input_size, output_size) # get weight from customized model\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iter = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67edda04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.621\n",
      "epoch 1: w = 0.497, loss = 27.99979\n",
      "epoch 2: w = 0.738, loss = 19.42999\n",
      "epoch 3: w = 0.939, loss = 13.48358\n",
      "epoch 4: w = 1.107, loss = 9.35749\n",
      "epoch 5: w = 1.246, loss = 6.49447\n",
      "epoch 6: w = 1.362, loss = 4.50787\n",
      "epoch 7: w = 1.459, loss = 3.12940\n",
      "epoch 8: w = 1.540, loss = 2.17290\n",
      "epoch 9: w = 1.607, loss = 1.50920\n",
      "epoch 10: w = 1.663, loss = 1.04866\n",
      "epoch 11: w = 1.710, loss = 0.72910\n",
      "epoch 12: w = 1.749, loss = 0.50735\n",
      "epoch 13: w = 1.782, loss = 0.35347\n",
      "epoch 14: w = 1.809, loss = 0.24669\n",
      "epoch 15: w = 1.831, loss = 0.17259\n",
      "epoch 16: w = 1.850, loss = 0.12117\n",
      "epoch 17: w = 1.866, loss = 0.08548\n",
      "epoch 18: w = 1.879, loss = 0.06070\n",
      "epoch 19: w = 1.890, loss = 0.04350\n",
      "epoch 20: w = 1.899, loss = 0.03156\n",
      "epoch 21: w = 1.907, loss = 0.02327\n",
      "epoch 22: w = 1.913, loss = 0.01750\n",
      "epoch 23: w = 1.919, loss = 0.01350\n",
      "epoch 24: w = 1.923, loss = 0.01071\n",
      "epoch 25: w = 1.927, loss = 0.00876\n",
      "epoch 26: w = 1.930, loss = 0.00741\n",
      "epoch 27: w = 1.933, loss = 0.00646\n",
      "epoch 28: w = 1.935, loss = 0.00579\n",
      "epoch 29: w = 1.937, loss = 0.00532\n",
      "epoch 30: w = 1.938, loss = 0.00499\n",
      "epoch 31: w = 1.940, loss = 0.00475\n",
      "epoch 32: w = 1.941, loss = 0.00457\n",
      "epoch 33: w = 1.942, loss = 0.00445\n",
      "epoch 34: w = 1.943, loss = 0.00435\n",
      "epoch 35: w = 1.944, loss = 0.00428\n",
      "epoch 36: w = 1.944, loss = 0.00422\n",
      "epoch 37: w = 1.945, loss = 0.00417\n",
      "epoch 38: w = 1.945, loss = 0.00413\n",
      "epoch 39: w = 1.946, loss = 0.00409\n",
      "epoch 40: w = 1.946, loss = 0.00406\n",
      "epoch 41: w = 1.946, loss = 0.00403\n",
      "epoch 42: w = 1.947, loss = 0.00400\n",
      "epoch 43: w = 1.947, loss = 0.00397\n",
      "epoch 44: w = 1.947, loss = 0.00395\n",
      "epoch 45: w = 1.948, loss = 0.00392\n",
      "epoch 46: w = 1.948, loss = 0.00390\n",
      "epoch 47: w = 1.948, loss = 0.00388\n",
      "epoch 48: w = 1.948, loss = 0.00385\n",
      "epoch 49: w = 1.948, loss = 0.00383\n",
      "epoch 50: w = 1.949, loss = 0.00381\n",
      "epoch 51: w = 1.949, loss = 0.00378\n",
      "epoch 52: w = 1.949, loss = 0.00376\n",
      "epoch 53: w = 1.949, loss = 0.00374\n",
      "epoch 54: w = 1.949, loss = 0.00371\n",
      "epoch 55: w = 1.950, loss = 0.00369\n",
      "epoch 56: w = 1.950, loss = 0.00367\n",
      "epoch 57: w = 1.950, loss = 0.00365\n",
      "epoch 58: w = 1.950, loss = 0.00363\n",
      "epoch 59: w = 1.950, loss = 0.00360\n",
      "epoch 60: w = 1.950, loss = 0.00358\n",
      "epoch 61: w = 1.950, loss = 0.00356\n",
      "epoch 62: w = 1.951, loss = 0.00354\n",
      "epoch 63: w = 1.951, loss = 0.00352\n",
      "epoch 64: w = 1.951, loss = 0.00350\n",
      "epoch 65: w = 1.951, loss = 0.00348\n",
      "epoch 66: w = 1.951, loss = 0.00346\n",
      "epoch 67: w = 1.951, loss = 0.00344\n",
      "epoch 68: w = 1.951, loss = 0.00342\n",
      "epoch 69: w = 1.952, loss = 0.00340\n",
      "epoch 70: w = 1.952, loss = 0.00337\n",
      "epoch 71: w = 1.952, loss = 0.00335\n",
      "epoch 72: w = 1.952, loss = 0.00333\n",
      "epoch 73: w = 1.952, loss = 0.00331\n",
      "epoch 74: w = 1.952, loss = 0.00329\n",
      "epoch 75: w = 1.953, loss = 0.00328\n",
      "epoch 76: w = 1.953, loss = 0.00326\n",
      "epoch 77: w = 1.953, loss = 0.00324\n",
      "epoch 78: w = 1.953, loss = 0.00322\n",
      "epoch 79: w = 1.953, loss = 0.00320\n",
      "epoch 80: w = 1.953, loss = 0.00318\n",
      "epoch 81: w = 1.953, loss = 0.00316\n",
      "epoch 82: w = 1.953, loss = 0.00314\n",
      "epoch 83: w = 1.954, loss = 0.00312\n",
      "epoch 84: w = 1.954, loss = 0.00310\n",
      "epoch 85: w = 1.954, loss = 0.00308\n",
      "epoch 86: w = 1.954, loss = 0.00307\n",
      "epoch 87: w = 1.954, loss = 0.00305\n",
      "epoch 88: w = 1.954, loss = 0.00303\n",
      "epoch 89: w = 1.954, loss = 0.00301\n",
      "epoch 90: w = 1.955, loss = 0.00299\n",
      "epoch 91: w = 1.955, loss = 0.00298\n",
      "epoch 92: w = 1.955, loss = 0.00296\n",
      "epoch 93: w = 1.955, loss = 0.00294\n",
      "epoch 94: w = 1.955, loss = 0.00292\n",
      "epoch 95: w = 1.955, loss = 0.00290\n",
      "epoch 96: w = 1.955, loss = 0.00289\n",
      "epoch 97: w = 1.956, loss = 0.00287\n",
      "epoch 98: w = 1.956, loss = 0.00285\n",
      "epoch 99: w = 1.956, loss = 0.00284\n",
      "epoch 100: w = 1.956, loss = 0.00282\n",
      "Prediction after training: f(5) = 9.909\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    # forward pass\n",
    "    Y_pred = model(X)\n",
    "    l = loss(Y,Y_pred)\n",
    "    # backward pass\n",
    "    l.backward()\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    # zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch%1 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.5f}')\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b2890",
   "metadata": {},
   "source": [
    "### sklearn dataset 實作 linear regression 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24cb93d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0502bba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.5539e+01, -1.0662e+01,  2.2757e+01,  1.0110e+02,  1.4434e+02,\n",
      "         3.3289e+01,  3.3015e+01, -2.5887e+01, -9.9639e+01,  2.3803e+01,\n",
      "        -4.5589e+01, -8.3388e+00, -9.5315e+01,  3.6407e+01, -8.7293e+01,\n",
      "         6.7669e+01, -1.3687e+01, -5.5441e+01, -6.5340e+01, -5.4450e+01,\n",
      "        -2.8835e+01,  1.7884e+02,  6.5084e+01,  2.6668e+01, -1.8546e+01,\n",
      "        -4.1499e+01,  8.5583e-01,  4.4562e+01,  1.1598e+02, -6.4620e+01,\n",
      "        -2.5931e+01, -6.0882e+01,  1.8720e+01,  7.5070e+01,  1.1720e+02,\n",
      "        -2.2698e+01, -5.6363e+01,  1.8084e+02, -1.9257e+02,  6.8503e+01,\n",
      "         1.6552e+02,  1.0500e+02, -7.0434e+01, -5.8769e+01, -4.1576e+01,\n",
      "         7.3247e+01,  4.0966e+01,  8.0462e+01, -2.8794e+01,  3.4234e+01,\n",
      "        -4.1715e+01,  1.4355e+01,  7.9336e+01,  2.7129e+01, -3.9487e+01,\n",
      "         6.6805e+01,  9.5531e+01,  3.5610e+00,  1.0857e-01,  5.6495e+01,\n",
      "         5.1575e+01, -2.0974e+00, -2.6656e+01,  3.9742e+01,  3.6101e+01,\n",
      "        -7.5602e+01,  1.9713e+01, -7.1601e+01, -1.9904e+01, -7.6708e+01,\n",
      "        -1.1834e+02, -2.9825e+01,  1.5108e+02,  5.2923e+01, -5.9552e+01,\n",
      "         3.0721e+01, -2.9355e+01, -4.4786e+01,  1.0006e+02,  1.5058e+02,\n",
      "         1.2200e+02, -1.8186e+02,  3.4739e+00, -2.2980e+01,  4.5184e+01,\n",
      "         9.8606e+01, -9.2779e+00, -5.2478e+01,  3.8593e+01, -1.9997e+02,\n",
      "        -9.5201e+00, -3.4724e+00, -3.5312e+01,  7.5406e+01,  1.7570e+01,\n",
      "        -2.3960e+01,  1.3209e+02,  2.0608e+01,  5.1111e+01, -2.6306e+01])\n",
      "tensor([[-5.5539e+01],\n",
      "        [-1.0662e+01],\n",
      "        [ 2.2757e+01],\n",
      "        [ 1.0110e+02],\n",
      "        [ 1.4434e+02],\n",
      "        [ 3.3289e+01],\n",
      "        [ 3.3015e+01],\n",
      "        [-2.5887e+01],\n",
      "        [-9.9639e+01],\n",
      "        [ 2.3803e+01],\n",
      "        [-4.5589e+01],\n",
      "        [-8.3388e+00],\n",
      "        [-9.5315e+01],\n",
      "        [ 3.6407e+01],\n",
      "        [-8.7293e+01],\n",
      "        [ 6.7669e+01],\n",
      "        [-1.3687e+01],\n",
      "        [-5.5441e+01],\n",
      "        [-6.5340e+01],\n",
      "        [-5.4450e+01],\n",
      "        [-2.8835e+01],\n",
      "        [ 1.7884e+02],\n",
      "        [ 6.5084e+01],\n",
      "        [ 2.6668e+01],\n",
      "        [-1.8546e+01],\n",
      "        [-4.1499e+01],\n",
      "        [ 8.5583e-01],\n",
      "        [ 4.4562e+01],\n",
      "        [ 1.1598e+02],\n",
      "        [-6.4620e+01],\n",
      "        [-2.5931e+01],\n",
      "        [-6.0882e+01],\n",
      "        [ 1.8720e+01],\n",
      "        [ 7.5070e+01],\n",
      "        [ 1.1720e+02],\n",
      "        [-2.2698e+01],\n",
      "        [-5.6363e+01],\n",
      "        [ 1.8084e+02],\n",
      "        [-1.9257e+02],\n",
      "        [ 6.8503e+01],\n",
      "        [ 1.6552e+02],\n",
      "        [ 1.0500e+02],\n",
      "        [-7.0434e+01],\n",
      "        [-5.8769e+01],\n",
      "        [-4.1576e+01],\n",
      "        [ 7.3247e+01],\n",
      "        [ 4.0966e+01],\n",
      "        [ 8.0462e+01],\n",
      "        [-2.8794e+01],\n",
      "        [ 3.4234e+01],\n",
      "        [-4.1715e+01],\n",
      "        [ 1.4355e+01],\n",
      "        [ 7.9336e+01],\n",
      "        [ 2.7129e+01],\n",
      "        [-3.9487e+01],\n",
      "        [ 6.6805e+01],\n",
      "        [ 9.5531e+01],\n",
      "        [ 3.5610e+00],\n",
      "        [ 1.0857e-01],\n",
      "        [ 5.6495e+01],\n",
      "        [ 5.1575e+01],\n",
      "        [-2.0974e+00],\n",
      "        [-2.6656e+01],\n",
      "        [ 3.9742e+01],\n",
      "        [ 3.6101e+01],\n",
      "        [-7.5602e+01],\n",
      "        [ 1.9713e+01],\n",
      "        [-7.1601e+01],\n",
      "        [-1.9904e+01],\n",
      "        [-7.6708e+01],\n",
      "        [-1.1834e+02],\n",
      "        [-2.9825e+01],\n",
      "        [ 1.5108e+02],\n",
      "        [ 5.2923e+01],\n",
      "        [-5.9552e+01],\n",
      "        [ 3.0721e+01],\n",
      "        [-2.9355e+01],\n",
      "        [-4.4786e+01],\n",
      "        [ 1.0006e+02],\n",
      "        [ 1.5058e+02],\n",
      "        [ 1.2200e+02],\n",
      "        [-1.8186e+02],\n",
      "        [ 3.4739e+00],\n",
      "        [-2.2980e+01],\n",
      "        [ 4.5184e+01],\n",
      "        [ 9.8606e+01],\n",
      "        [-9.2779e+00],\n",
      "        [-5.2478e+01],\n",
      "        [ 3.8593e+01],\n",
      "        [-1.9997e+02],\n",
      "        [-9.5201e+00],\n",
      "        [-3.4724e+00],\n",
      "        [-3.5312e+01],\n",
      "        [ 7.5406e+01],\n",
      "        [ 1.7570e+01],\n",
      "        [-2.3960e+01],\n",
      "        [ 1.3209e+02],\n",
      "        [ 2.0608e+01],\n",
      "        [ 5.1111e+01],\n",
      "        [-2.6306e+01]])\n",
      "100 1\n"
     ]
    }
   ],
   "source": [
    "# 0. data preparation\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "print(y)\n",
    "y = y.view(y.shape[0], 1)\n",
    "print(y)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8efee1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss = 4475.8857\n",
      "epoch 20, loss = 3338.3359\n",
      "epoch 30, loss = 2515.0002\n",
      "epoch 40, loss = 1918.4633\n",
      "epoch 50, loss = 1485.8331\n",
      "epoch 60, loss = 1171.7933\n",
      "epoch 70, loss = 943.6487\n",
      "epoch 80, loss = 777.7801\n",
      "epoch 90, loss = 657.1039\n",
      "epoch 100, loss = 569.2502\n"
     ]
    }
   ],
   "source": [
    "# 1. define model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2. define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3. training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weight\n",
    "    optimizer.step()\n",
    "    \n",
    "    # zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1)%10 ==0:\n",
    "        print(f'epoch {epoch+1}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5aa0f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a6dd0150d0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "predicted = model(X).detach()\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feadaf1",
   "metadata": {},
   "source": [
    "### sklearn dataset 實作 logistic regression 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a00a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17b98913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n"
     ]
    }
   ],
   "source": [
    "# 0. data preparation\n",
    "data_bc = datasets.load_breast_cancer()\n",
    "X, y = data_bc.data, data_bc.target\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# data scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# turn into tensor\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "525b628d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss = 0.5011\n",
      "epoch 20, loss = 0.4250\n",
      "epoch 30, loss = 0.3745\n",
      "epoch 40, loss = 0.3382\n",
      "epoch 50, loss = 0.3106\n",
      "epoch 60, loss = 0.2889\n",
      "epoch 70, loss = 0.2712\n",
      "epoch 80, loss = 0.2564\n",
      "epoch 90, loss = 0.2439\n",
      "epoch 100, loss = 0.2330\n",
      "accuracy = 0.89474\n"
     ]
    }
   ],
   "source": [
    "# 1. define model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "learning_rate = 0.01\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# 2. define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3. training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weight\n",
    "    optimizer.step()\n",
    "    \n",
    "    # zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1)%10 ==0:\n",
    "        print(f'epoch {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# not to track gradient calculations\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred_binary = y_pred.round()\n",
    "    acc = y_pred_binary.eq(y_test).sum() / float(y_test.shape[0]) # y_pred_binary==y_test 時計數加一\n",
    "    print(f'accuracy = {acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472952c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_torch",
   "language": "python",
   "name": "py39_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
